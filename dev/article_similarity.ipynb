{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/lp/3p8p8yx17vs3nf76dxw6kx_m0000gn/T/jieba.cache\n",
      "Loading model cost 0.861 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('user_defined_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNNC': 'n', 'unnc': 'n', '宁诺': 'n', '诺圈': 'n', 'Nottinghome': 'n', 'nottinghome': 'n'}\n"
     ]
    }
   ],
   "source": [
    "print(jieba.user_word_tag_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../weiboData.xlsx', 'sheet1', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in data.iterrows():\n",
    "#     data.loc[index, 'seg'] = re.sub('\\u3000', '', data.loc[index, 'text'])\n",
    "#     data.loc[index, 'seg'] = re.sub('\\s+', ' ', data.loc[index, 'text'])\n",
    "#     data.loc[index, 'seg'] = re.sub('[a-zA-Z0-9’!\"#$·_－＿＊%&\\'()*+,-./:;<=>?@：（），。?★、…【】《》？“”‘’！[\\\\]^_`{|}~\\s]+', \" \", data.loc[index, 'text'])\n",
    "    \n",
    "# data.head()\n",
    "# data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(path=r'./停用词汇总.txt'):\n",
    "    file = open(path, 'r', encoding='utf-8').read().split('\\n')\n",
    "    return set(file)\n",
    "\n",
    "def rm_tokens(words, stwlist):\n",
    "    words_list = list(words)\n",
    "    stop_words = stwlist\n",
    "    for i in range(words_list.__len__())[::-1]:\n",
    "        # 去除停用词\n",
    "        if words_list[i] in stop_words:\n",
    "            words_list.pop(i)\n",
    "        # 去除数字\n",
    "        elif words_list[i].isdigit():\n",
    "            words_list.pop(i)\n",
    "        # 去除单个字符\n",
    "        elif len(words_list[i]) == 1:\n",
    "            words_list.pop(i)\n",
    "        # 去除空格\n",
    "        elif words_list[i] == \" \":\n",
    "            words_list.pop(i)\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stwlist = get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.enable_parallel(64)\n",
    "data['seg'] = data['text'].apply(lambda i : rm_tokens(jieba.cut(i, cut_all=False), stwlist))\n",
    "# data['seg'] =[' '.join(i) for i in data['seg']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = []\n",
    "str_list = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     8.21中午求拼车南安普顿大学二手社区\n",
       "1       如今二手房市场惨淡的原因你知道吗？这是否反映了当今楼市的残酷现实？#二手房#涨知识##快手#...\n",
       "2          伊犁理论上说，每卖出一套新房，就会多出一套二手房。但是二手房价似乎一点都没跌啊？你有感觉么？\n",
       "3                #美国##我的vlog生活##福特野马#买二手车要砍价！L小水水iii的微博视频\n",
       "4       周二休息周一晚就跑去鱼鱼家度假叻测试完麦克风和二手音响都没毛病之后就困困了抱着小恐龙等鱼鱼下...\n",
       "                              ...                        \n",
       "7874    #房产微盘点#最近，南京高考成绩出炉，不少学区房家长开始新一轮焦虑，学区房一直是社会热点，但...\n",
       "7875    #粉笔挖媒#今日聚焦社会热点——屡被质疑的青霉面包做汉堡，道歉不能代替严惩。小粉笔和你一起进...\n",
       "7876    【老人摔倒敢不敢扶？遇到小偷敢不敢追？民事检察官这样说……】近年来，老人倒地扶不扶、遇见小偷...\n",
       "7877    #社会热点#据媒体报道，腾讯将牵头合并斗鱼和虎牙的谈判。据新京报6月11日报道，腾讯正推动斗...\n",
       "7878    @秦淮发布#民法典与百姓生活#【老人摔倒敢不敢扶？遇到小偷敢不敢追？民事检察官这样说……】近...\n",
       "Name: text, Length: 7879, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = str_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list=[]\n",
    "for sentence in corpus_list:\n",
    "    sentence_list=[ word for word in rm_tokens(jieba.cut(sentence, cut_all=False), stwlist)]\n",
    "    texts_list.append(sentence_list)\n",
    "    \n",
    "dictionary=gensim.corpora.Dictionary(texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(253 unique tokens: ['8.21', '中午', '二手', '南安普顿', '大学']...)\n",
      "{'8.21': 0, '中午': 1, '二手': 2, '南安普顿': 3, '大学': 4, '拼车': 5, '社区': 6, '二手房': 7, '当今': 8, '快手': 9, '惨淡': 10, '楼市': 11, '残酷': 12, '现实': 13, '白话': 14, '视频': 15, '闲哥': 16, '一套': 17, '伊犁': 18, '卖出': 19, '多出': 20, '新房': 21, '理论': 22, 'iii': 23, 'vlog': 24, '二手车': 25, '小水水': 26, '微博': 27, '生活': 28, '砍价': 29, '福特': 30, '美国': 31, '野马': 32, '下班': 33, '休息': 34, '午觉': 35, '半夜': 36, '周一': 37, '周二': 38, '困困': 39, '差点': 40, '度假': 41, '开封': 42, '恐龙': 43, '抽走': 44, '新品': 45, '早茶': 46, '毛病': 47, '测试': 48, '电影': 49, '电影票': 50, '睡着': 51, '起不来': 52, '还好': 53, '进到': 54, '迟到': 55, '迷迷糊糊': 56, '隔壁': 57, '音响': 58, '顺便': 59, '鱼家': 60, '麦克风': 61, 'BfiCTCiSxa8': 62, '两江': 63, '四岸': 64, '房产': 65, '抖音': 66, '看房': 67, '重庆': 68, 'FJ40': 69, '兰德': 70, '改装': 71, '疯狂': 72, '越野': 73, '部落': 74, '酷路泽': 75, 'To': 76, 'pb': 77, '一颗': 78, '三点': 79, '不好': 80, '两份': 81, '互动': 82, '凌晨': 83, '去年': 84, '后悔': 85, '周边': 86, '夏天': 87, '将来': 88, '年轻': 89, '床上': 90, '心碎': 91, '感叹': 92, '成渣': 93, '死死的': 94, '生日': 95, '留恋': 96, '留点': 97, '祝福': 98, '空调': 99, '纪念品': 100, '随便': 101, 'TF': 102, 'YSL': 103, '价钱': 104, '别来': 105, '新色': 106, '海淘求': 107, '货团': 108, '贩子': 109, '阿玛尼': 110, '三室一厅': 111, '关爱': 112, '创意': 113, '动手': 114, '吹倒': 115, '太强': 116, '山景': 117, '幽默': 118, '房子': 119, '搞笑': 120, '改成': 121, '沙雕': 122, '没钱买': 123, '波音': 124, '浴缸': 125, '美元': 126, '胡椒': 127, '这架': 128, '阿姨': 129, '飓风': 130, '飞机': 131, '驾驶舱': 132, '五菱': 133, '求购': 134, '面包车': 135, '243.5': 136, '5.5': 137, 'NBA': 138, '中超': 139, '主推': 140, '全场': 141, '全红': 142, '分享': 143, '季后赛': 144, '明天': 145, '澳甲': 146, '火箭': 147, '点赞点': 148, '独行侠': 149, '赞点': 150, '二手货': 151, '心烧': 152, '意想不到': 153, '贪平': 154, '过程': 155, '酸楚': 156, 'lucky': 157, '二手手机': 158, '五行': 159, '关键': 160, '卖个': 161, '朋友': 162, '第一次': 163, '三明': 164, '书中': 165, '传播学院': 166, '体育': 167, '信工': 168, '化工学院': 169, '团委': 170, '外国语': 171, '大众': 172, '学院': 173, '工程学院': 174, '平凡': 175, '康养': 176, '忍受': 177, '感兴趣': 178, '教育': 179, '数不清': 180, '文化': 181, '时间': 182, '晚安': 183, '普罗': 184, '机电': 185, '没人': 186, '管理': 187, '经济': 188, '苦难': 189, '资源': 190, '阿列克谢耶维奇': 191, '音乐学院': 192, '默默无闻': 193, '一千块': 194, '一发': 195, '买个': 196, '笔记本': 197, '钱能': 198, '全网': 199, '教案': 200, '顶尖': 201, '高中语文': 202, '出行': 203, '标致': 204, '瓦罐': 205, '路上': 206, '一两点': 207, '二手烟': 208, '八月': 209, '几天': 210, '加重': 211, '呼吸': 212, '啊啊啊': 213, '喉咙': 214, '嘴巴': 215, '嘶哑': 216, '声音': 217, '复发': 218, '太过分': 219, '完醒': 220, '导致': 221, '异物': 222, '必做': 223, '慢性': 224, '支气管炎': 225, '早痛': 226, '晚上': 227, '每天晚上': 228, '气死我了': 229, '油死': 230, '洗漱': 231, '痘痘': 232, '痛苦': 233, '睡觉': 234, '脸上': 235, '难看': 236, '难过': 237, '顺畅': 238, '鼻孔': 239, '鼻炎': 240, '320Li': 241, '上班': 242, '合算': 243, '宝藏': 244, '宝马': 245, '小哥': 246, '换车': 247, '支付宝': 248, '极品': 249, '档次': 250, '汽车': 251, '羡慕': 252}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['8.21', '中午', '拼车', '南安普顿', '大学', '二手', '社区'],\n",
       " ['二手房',\n",
       "  '惨淡',\n",
       "  '当今',\n",
       "  '楼市',\n",
       "  '残酷',\n",
       "  '现实',\n",
       "  '二手房',\n",
       "  '快手',\n",
       "  '快手',\n",
       "  '闲哥',\n",
       "  '白话',\n",
       "  '快手',\n",
       "  '视频'],\n",
       " ['伊犁', '理论', '卖出', '一套', '新房', '多出', '一套', '二手房', '二手房'],\n",
       " ['美国', 'vlog', '生活', '福特', '野马', '二手车', '砍价', '小水水', 'iii', '微博', '视频'],\n",
       " ['周二',\n",
       "  '休息',\n",
       "  '周一',\n",
       "  '鱼家',\n",
       "  '度假',\n",
       "  '测试',\n",
       "  '麦克风',\n",
       "  '二手',\n",
       "  '音响',\n",
       "  '毛病',\n",
       "  '困困',\n",
       "  '恐龙',\n",
       "  '下班',\n",
       "  '迷迷糊糊',\n",
       "  '睡着',\n",
       "  '半夜',\n",
       "  '恐龙',\n",
       "  '抽走',\n",
       "  '隔壁',\n",
       "  '进到',\n",
       "  '中午',\n",
       "  '早茶',\n",
       "  '顺便',\n",
       "  '电影票',\n",
       "  '午觉',\n",
       "  '起不来',\n",
       "  '差点',\n",
       "  '迟到',\n",
       "  '还好',\n",
       "  '电影',\n",
       "  '电影',\n",
       "  '开封',\n",
       "  '新品'],\n",
       " ['重庆', '房产', '二手房', '两江', '四岸', '房产', '看房', 'BfiCTCiSxa8', '抖音'],\n",
       " ['改装', '越野', '部落', 'FJ40', '兰德', '酷路泽', '改装', '疯狂', '二手车'],\n",
       " ['去年',\n",
       "  '空调',\n",
       "  '凌晨',\n",
       "  '三点',\n",
       "  '床上',\n",
       "  '一颗',\n",
       "  '生日',\n",
       "  '祝福',\n",
       "  '互动',\n",
       "  '感叹',\n",
       "  '年轻',\n",
       "  '死死的',\n",
       "  '后悔',\n",
       "  '两份',\n",
       "  'pb',\n",
       "  '留恋',\n",
       "  '二手',\n",
       "  '夏天',\n",
       "  '留点',\n",
       "  '纪念品',\n",
       "  '不好',\n",
       "  '心碎',\n",
       "  '成渣',\n",
       "  'To',\n",
       "  '将来',\n",
       "  '随便',\n",
       "  '周边'],\n",
       " ['海淘求', 'TF', '阿玛尼', 'YSL', '新色', '货团', '二手', '贩子', '别来', '价钱'],\n",
       " ['房子',\n",
       "  '飓风',\n",
       "  '吹倒',\n",
       "  '美国',\n",
       "  '阿姨',\n",
       "  '没钱买',\n",
       "  '新房',\n",
       "  '美元',\n",
       "  '二手',\n",
       "  '飞机',\n",
       "  '这架',\n",
       "  '波音',\n",
       "  '改成',\n",
       "  '三室一厅',\n",
       "  '山景',\n",
       "  '驾驶舱',\n",
       "  '浴缸',\n",
       "  '创意',\n",
       "  '动手',\n",
       "  '太强',\n",
       "  '胡椒',\n",
       "  '视频',\n",
       "  '搞笑',\n",
       "  '视频',\n",
       "  '幽默',\n",
       "  '关爱',\n",
       "  '沙雕',\n",
       "  '微博',\n",
       "  '视频'],\n",
       " ['求购', '二手', '五菱', '面包车'],\n",
       " ['澳甲',\n",
       "  '中超',\n",
       "  '全红',\n",
       "  '明天',\n",
       "  'NBA',\n",
       "  '分享',\n",
       "  '季后赛',\n",
       "  '火箭',\n",
       "  '全场',\n",
       "  '243.5',\n",
       "  '主推',\n",
       "  '二手',\n",
       "  '独行侠',\n",
       "  '5.5',\n",
       "  '点赞点',\n",
       "  '赞点'],\n",
       " ['贪平', '二手货', '过程', '意想不到', '酸楚', '心烧'],\n",
       " ['朋友', '五行', 'lucky', '卖个', '二手手机', '关键', '第一次'],\n",
       " ['晚安',\n",
       "  '信工',\n",
       "  '平凡',\n",
       "  '默默无闻',\n",
       "  '生活',\n",
       "  '忍受',\n",
       "  '苦难',\n",
       "  '没人',\n",
       "  '感兴趣',\n",
       "  '书中',\n",
       "  '普罗',\n",
       "  '大众',\n",
       "  '数不清',\n",
       "  '阿列克谢耶维奇',\n",
       "  '二手',\n",
       "  '时间',\n",
       "  '晚安',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '外国语',\n",
       "  '学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '机电',\n",
       "  '工程学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '资源',\n",
       "  '化工学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '文化',\n",
       "  '传播学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '教育',\n",
       "  '音乐学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '经济',\n",
       "  '管理',\n",
       "  '学院',\n",
       "  '团委',\n",
       "  '三明',\n",
       "  '学院',\n",
       "  '体育',\n",
       "  '康养',\n",
       "  '学院',\n",
       "  '团委'],\n",
       " ['一发', '一千块', '钱能', '买个', '笔记本', '二手'],\n",
       " ['全网', '二手', '高中语文', '顶尖', '教案'],\n",
       " ['标致', '瓦罐', '路上', '改装', '出行', '二手车'],\n",
       " ['几天',\n",
       "  '难过',\n",
       "  '鼻炎',\n",
       "  '加重',\n",
       "  '每天晚上',\n",
       "  '呼吸',\n",
       "  '顺畅',\n",
       "  '鼻孔',\n",
       "  '死死的',\n",
       "  '嘴巴',\n",
       "  '呼吸',\n",
       "  '慢性',\n",
       "  '支气管炎',\n",
       "  '二手烟',\n",
       "  '复发',\n",
       "  '导致',\n",
       "  '喉咙',\n",
       "  '早痛',\n",
       "  '异物',\n",
       "  '声音',\n",
       "  '嘶哑',\n",
       "  '脸上',\n",
       "  '痘痘',\n",
       "  '油死',\n",
       "  '难看',\n",
       "  '晚上',\n",
       "  '睡觉',\n",
       "  '必做',\n",
       "  '完醒',\n",
       "  '痛苦',\n",
       "  '不好',\n",
       "  '啊啊啊',\n",
       "  '下班',\n",
       "  '洗漱',\n",
       "  '一两点',\n",
       "  '气死我了',\n",
       "  '气死我了',\n",
       "  '气死我了',\n",
       "  '八月',\n",
       "  '太过分'],\n",
       " ['二手车',\n",
       "  '晚上',\n",
       "  '支付宝',\n",
       "  '上班',\n",
       "  '小哥',\n",
       "  '宝马',\n",
       "  '320Li',\n",
       "  '小哥',\n",
       "  '换车',\n",
       "  '档次',\n",
       "  '极品',\n",
       "  '宝马',\n",
       "  '320Li',\n",
       "  '合算',\n",
       "  '羡慕',\n",
       "  '宝藏',\n",
       "  '视频',\n",
       "  '汽车',\n",
       "  '微博',\n",
       "  '视频']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(7, 2), (8, 1), (9, 3), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(7, 2), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(15, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (2, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)], [(7, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1)], [(25, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1)], [(2, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1)], [(2, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1)], [(2, 1), (15, 3), (21, 1), (27, 1), (31, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1)], [(2, 1), (133, 1), (134, 1), (135, 1)], [(2, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1)], [(151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1)], [(157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1)], [(2, 1), (28, 1), (164, 8), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 8), (171, 1), (172, 1), (173, 11), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1)], [(2, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1)], [(2, 1), (199, 1), (200, 1), (201, 1), (202, 1)], [(25, 1), (71, 1), (203, 1), (204, 1), (205, 1), (206, 1)], [(33, 1), (80, 1), (94, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 2), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 3), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1)], [(15, 2), (25, 1), (27, 1), (227, 1), (241, 2), (242, 1), (243, 1), (244, 1), (245, 2), (246, 2), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in texts_list]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析测试文档与已存在的每个训练文本的相似度\n",
    "index = gensim.similarities.SparseMatrixSimilarity(tfidf[corpus], \n",
    "                                                   num_features=len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 使用tf-idf算法对上面的语料库进行建模，识别不同文本的相似度。\n",
    "similarity_matrix = []\n",
    "\n",
    "\n",
    "# for i in range(0, 3):\n",
    "#     test_string = data['text'][i]\n",
    "#     test_doc_list = [word for word in jieba.cut(test_string)]\n",
    "#     test_doc_vec = dictionary.doc2bow(test_doc_list)\n",
    "#     similarity[i] = index[tfidf[test_doc_vec]]\n",
    "\n",
    "print(similarity_matrix)\n",
    "# print(np.around(similarity, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.save(\"tfidf.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('matrixFile.txt', similarity, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_list = [word for word in jieba.cut(test_string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_vec = dictionary.doc2bow(test_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.9999999 , 0.11877163, 0.02388704, 0.        ,\n",
       "       0.05879446, 0.        , 0.        , 0.        , 0.03996632,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.02819041],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = index[tfidf[test_doc_vec]]\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.9999999 , 0.11877163, 0.02388704, 0.        ,\n",
       "        0.05879446, 0.        , 0.        , 0.        , 0.03996632,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.02819041],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = data['text'][0]\n",
    "test_doc_list = [word for word in jieba.cut(test_string)]\n",
    "test_doc_vec = dictionary.doc2bow(test_doc_list)\n",
    "similarity2 = index[tfidf[test_doc_vec]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.9999999 , 0.11877163, 0.02388704, 0.        ,\n",
       "        0.05879446, 0.        , 0.        , 0.        , 0.03996632,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.02819041],\n",
       "       dtype=float32),\n",
       " array([1.0000001 , 0.        , 0.        , 0.        , 0.04568831,\n",
       "        0.        , 0.        , 0.00448577, 0.00748911, 0.00439955,\n",
       "        0.01289549, 0.00580791, 0.        , 0.        , 0.00134752,\n",
       "        0.01002401, 0.01119238, 0.        , 0.        , 0.        ],\n",
       "       dtype=float32),\n",
       " array([1.0000001 , 0.        , 0.        , 0.        , 0.04568831,\n",
       "        0.        , 0.        , 0.00448577, 0.00748911, 0.00439955,\n",
       "        0.01289549, 0.00580791, 0.        , 0.        , 0.00134752,\n",
       "        0.01002401, 0.01119238, 0.        , 0.        , 0.        ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.append(similarity2)\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.mat(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.9999999 , 0.11877163, 0.02388704, 0.        ,\n",
       "         0.05879446, 0.        , 0.        , 0.        , 0.03996632,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.02819041],\n",
       "        [1.0000001 , 0.        , 0.        , 0.        , 0.04568831,\n",
       "         0.        , 0.        , 0.00448577, 0.00748911, 0.00439955,\n",
       "         0.01289549, 0.00580791, 0.        , 0.        , 0.00134752,\n",
       "         0.01002401, 0.01119238, 0.        , 0.        , 0.        ],\n",
       "        [1.0000001 , 0.        , 0.        , 0.        , 0.04568831,\n",
       "         0.        , 0.        , 0.00448577, 0.00748911, 0.00439955,\n",
       "         0.01289549, 0.00580791, 0.        , 0.        , 0.00134752,\n",
       "         0.01002401, 0.01119238, 0.        , 0.        , 0.        ]],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('matrixFile.txt', s, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
